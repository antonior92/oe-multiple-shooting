from __future__ import division, print_function, absolute_import
import numpy as np
from simident.neuralnetwork import (FeedforwardNetwork, LeCunInitializer,
                                    Logistic, scale_input_layer,
                                    scale_output_layer)
from numpy.testing import (TestCase, assert_array_almost_equal,
                           assert_array_equal, assert_array_less,
                           assert_raises, assert_equal, assert_,
                           run_module_suite, assert_allclose, assert_warns,
                           dec)


class TestLeCunInitializer(TestCase):

    def test_raise_exception(self):

        assert_raises(ValueError, LeCunInitializer, distribution='blab')
        assert_raises(ValueError, LeCunInitializer, gain=-1)
        assert_raises(ValueError, LeCunInitializer, gain=0)

    def test_shape_check(self):

        net = FeedforwardNetwork(ninput=3, noutput=2,
                                 nhidden=[2, 3],
                                 activ_func=Logistic())
        weights, bias = LeCunInitializer()(net)

        # Check shape
        assert_equal(weights[0].shape, (2, 3))
        assert_equal(weights[1].shape, (3, 2))
        assert_equal(weights[2].shape, (2, 3))
        assert_equal(bias[0].shape, (2, 1))
        assert_equal(bias[1].shape, (3, 1))
        assert_equal(bias[2].shape, (2, 1))

    def test_normal_distribution_std_check(self):

        nexp = 1000

        net = FeedforwardNetwork(ninput=3, noutput=2,
                                 nhidden=[2, 3],
                                 activ_func=Logistic())

        mean_per_layer = np.zeros(net.nlayers)
        variance_per_layer = np.zeros(net.nlayers)
        for i in range(nexp):

            np.random.seed(i)
            weights, bias = LeCunInitializer()(net)

            for j in range(net.nlayers):
                w = weights[j]
                we = w.shape[0]*w.shape[1]
                mean_per_layer[j] += 1/(nexp*we)*np.sum(w)
                variance_per_layer[j] += 1/(nexp*we)*np.sum(w**2)

        std_per_layer = np.sqrt(variance_per_layer)

        assert_array_almost_equal(std_per_layer, [1/np.sqrt(3),
                                                  1/np.sqrt(2),
                                                  1/np.sqrt(3)], decimal=2)
        assert_array_almost_equal(mean_per_layer, [0, 0, 0], decimal=2)

    def test_uniform_distribution_std_check(self):

        nexp = 1000

        net = FeedforwardNetwork(ninput=3, noutput=2,
                                 nhidden=[2, 3],
                                 activ_func=Logistic())

        mean_per_layer = np.zeros(net.nlayers)
        variance_per_layer = np.zeros(net.nlayers)
        for i in range(nexp):

            np.random.seed(i*2)
            weights, bias = LeCunInitializer(distribution='uniform')(net)

            for j in range(net.nlayers):
                w = weights[j]
                we = w.shape[0]*w.shape[1]
                mean_per_layer[j] += 1/(nexp*we)*np.sum(w)
                variance_per_layer[j] += 1/(nexp*we)*np.sum(w**2)

        std_per_layer = np.sqrt(variance_per_layer)

        assert_array_almost_equal(std_per_layer, [1/np.sqrt(3),
                                                  1/np.sqrt(2),
                                                  1/np.sqrt(3)], decimal=2)
        assert_array_almost_equal(mean_per_layer, [0, 0, 0], decimal=2)


class TestScaleInputLayer(TestCase):

    def test_scale_input_layer(self):

        ub = [10, -10, 5]
        lb = [5, -20, -7]
        weights = [np.ones((2, 3)), np.eye(2), np.zeros((3, 2))]
        bias = [np.zeros((2, )), np.ones((2,)), np.ones((3,))]
        new_weights, new_bias = scale_input_layer(weights, bias, ub, lb)
        W = new_weights[0]
        b = new_bias[0]

        assert_array_almost_equal(W.dot([10, -10,  5])+b, [3, 3])
        assert_array_almost_equal(W.dot([5, -20,  -7])+b, [-3, -3])
        assert_array_almost_equal(new_weights[1], weights[1])
        assert_array_almost_equal(new_weights[2], weights[2])
        assert_array_almost_equal(new_bias[1], bias[1])
        assert_array_almost_equal(new_bias[2], bias[2])


class TestScaleOutputLayer(TestCase):

    def test_scale_output_layer(self):

        ub = [10, -10]
        lb = [5, -20]
        weights = [np.ones((3, 3)), np.eye(3), np.ones((2, 3))]
        bias = [np.zeros((3, )), np.ones((3,)), np.zeros((2,))]
        new_weights, new_bias \
            = scale_output_layer(weights, bias, ub, lb)
        W = new_weights[-1]
        b = new_bias[-1]

        assert_array_almost_equal(W.dot([1, 0, 0])+b, ub)
        assert_array_almost_equal(W.dot([0, 1, 0])+b, ub)
        assert_array_almost_equal(W.dot([0, 0, 1])+b, ub)
        assert_array_almost_equal(W.dot([-1, 0, 0])+b, lb)
        assert_array_almost_equal(W.dot([0, -1, 0])+b, lb)
        assert_array_almost_equal(W.dot([0, 0, -1])+b, lb)
        assert_array_almost_equal(new_weights[1], weights[1])
        assert_array_almost_equal(new_weights[0], weights[0])
        assert_array_almost_equal(new_bias[1], bias[1])
        assert_array_almost_equal(new_bias[0], bias[0])
